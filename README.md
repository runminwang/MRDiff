# MRDiff
Get More with Less: Ultrasound Image-mask Pairs Synthesis through Diffusion Model

Breast cancer is the most prevalent cancer among women worldwide and has been a significant public health concern for many years. Segmentation and recognition of breast ultrasound images based on data-driven deep learning methods will greatly contribute to more accurate diagnosis. However, issues such as privacy and the high cost of annotating masks limit the size of the dataset of breast ultrasound images, and thus the performance improvement of breast tumour segmentation models. To address these issues, we propose a framework for the generation of breast ultrasound image-mask pairs based on the diffusion model. Specifically, in order to generate realistic breast ultrasound images, we propose MRDiff, which introduces semantic information in the mask into the generation process by the CEResblock. Then, to avoid the necessity of manual annotations, we design a mask generation module, which using an unconditional diffusion model to generate breast tumor mask. In comparison to the state-of-the-art methods on three breast ultrasound datasets and one X-ray dataset, the images generated  demonstrate the superiority of our approach. Furthermore, it is demonstrated that the data augmentation achieved through our approach contribute to an improvement in the accuracy of downstream segmentation tasks. The code will be released after the paper is published.
